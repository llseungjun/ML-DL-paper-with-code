# Seq2Seq(Sequence to Sequence Learning with Neural Networks)
> Paper : [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215)

## 1. Introduction
이 논문에서는 시퀀스를 시퀀스로 변환하는 엔드투엔드 LSTM 모델을 제안한다. 모델은 입력 시퀀스를 벡터로 인코딩한 후, 이를 기반으로 타겟 시퀀스를 디코딩한다. WMT'14 영어-프랑스어 번역 실험에서 제안된 LSTM 모델은 BLEU 점수 34.8을 기록하며, 기존 phrase-based SMT 시스템(33.3점)보다 높은 성능을 보였다. 또한, LSTM을 사용해 SMT 시스템의 결과를 재정렬하면 BLEU 점수가 36.5까지 향상되었다. LSTM은 단어 순서에 민감하면서도 능동태와 수동태 변환에는 강건한 표현을 학습했으며, 입력 문장의 단어 순서를 반대로 배열하면 성능이 향상되는 현상이 관찰되었다.

## 2. The model

## 3. Experiment

### 3.1. Dataset details

### 3.2. Decoding and Rescoring

### 3.3. Reversing the Source Sentences

### 3.4. Training details

### 3.5. Parallelization

### 3.6. Experimental Results

### 3.7. Performance on long sentences



### 후기  
**느낀점**     

**아쉬운점**  

**배운점**  
- 논문의 주요 구조(abstract, introduction, experiment 등)에 대해 구조화하면서 읽는 것에 대한 필요성을 느꼈다.(구조화 -> 요약 순서대로 내용 정리)  

<details>
<summary>Abstract 구조화</summary>

#### 1. 연구 목표 (What & Why)
- 이 논문은 어떤 문제를 해결하려고 하는가?
- 기존 연구의 한계는 무엇인가?
- 연구의 핵심 기여(contribution)는 무엇인가?

#### 2. 제안된 방법 (How)
- 어떤 접근법을 사용했는가?
- 모델의 구조나 방법론의 특징은 무엇인가?

#### 3. 실험 및 결과 (Does it work?)
- 연구에서 수행한 실험은 무엇인가?
- 제안된 방법의 성능은 어떠한가?
- 어떤 평가 지표를 사용했으며, 기존 방법과 비교했을 때 얼마나 성능이 향상되었는가?

#### 4. 비교 및 의의 (Comparison & Contribution)
- 기존 방법과 비교했을 때 어떤 점이 개선되었는가?
- 연구 결과가 어떤 의미를 가지는가?

#### 5. 흥미로운 추가 발견 (Interesting Insights)
- 연구 과정에서 발견한 흥미로운 점은 있는가?
- 연구자가 강조하는 추가적인 통찰(insight)이 있는가?

📖 이 방법을 적용해 초록 읽기 예시

✅ 연구 목표:

DNNs는 대량의 라벨링 데이터가 있을 때 강력하지만, 시퀀스를 시퀀스로 변환하는 데 한계가 있다.
이를 해결하기 위해 시퀀스 학습을 위한 엔드투엔드 접근법을 제안한다.

✅ 제안된 방법:

멀티레이어 LSTM을 사용해 입력 시퀀스를 벡터로 변환하고, 다른 LSTM이 이를 기반으로 타겟 시퀀스를 생성한다.

✅ 실험 및 결과:

WMT’14 영어-프랑스어 번역에서 BLEU 점수 34.8을 달성했다.
기존의 phrase-based SMT 시스템(33.3점)보다 높은 성능을 기록했다.
LSTM을 SMT 결과를 재정렬하는 데 활용하면 BLEU 점수가 36.5로 향상되었다.

✅ 비교 및 의의:

LSTM이 단어 순서에 민감하면서도 능동태/수동태 변환에는 상대적으로 불변한 문장 표현을 학습했다.
입력 문장의 단어 순서를 뒤집으면 LSTM의 학습 성능이 향상되었다는 점이 발견되었다.  

</details>

<details>
<summary>Abstract 요약</summary>
구조화된 내용을 잘 요약하려면 핵심 정보만 남기고, 간결하면서도 논문의 기여를 명확히 드러내는 방식으로 정리하는 게 중요해. 이를 위해 다음과 같은 방법을 사용할 수 있어.

---

✍️ 효과적인 요약 방법
1. 핵심 내용만 남기기

- 세부적인 수치나 비교 결과는 줄이고, 가장 중요한 성과만 남긴다.
- 불필요한 배경 설명이나 일반적인 내용은 제거한다.

2. 논문의 기여가 드러나도록 서술하기

- "이 논문에서는..." 같은 표현을 사용해 연구의 기여를 강조한다.
- 연구 목표 → 방법 → 결과 → 의의의 흐름을 유지한다.

3. 문장을 짧고 명확하게 정리하기

- 복잡한 문장은 끊어서 정리하고, 한 문장에 하나의 핵심 내용만 포함한다.
- 예: "본 연구에서는 LSTM을 활용한 시퀀스 변환 모델을 제안하고, 번역 성능이 기존 SMT보다 우수함을 보였다."

📖 초록 요약 예시
이 논문에서는 시퀀스를 시퀀스로 변환하는 엔드투엔드 LSTM 모델을 제안한다. 모델은 입력 시퀀스를 벡터로 인코딩한 후, 이를 기반으로 타겟 시퀀스를 디코딩한다. WMT'14 영어-프랑스어 번역 실험에서 제안된 LSTM 모델은 BLEU 점수 34.8을 기록하며, 기존 phrase-based SMT 시스템(33.3점)보다 높은 성능을 보였다. 또한, LSTM을 사용해 SMT 시스템의 결과를 재정렬하면 BLEU 점수가 36.5까지 향상되었다. LSTM은 단어 순서에 민감하면서도 능동태와 수동태 변환에는 강건한 표현을 학습했으며, 입력 문장의 단어 순서를 반대로 배열하면 성능이 향상되는 현상이 관찰되었다.

🔍 요약이 잘된 이유
✅ 핵심 내용만 포함 (세부 실험 설명, 필요 없는 배경 제거)
✅ 연구 목표 → 방법 → 결과 → 의의 순서 유지
✅ 문장을 짧고 명확하게 정리

이렇게 정리하면 논문의 주요 기여를 빠르고 효과적으로 전달할 수 있어! 🚀
</details>