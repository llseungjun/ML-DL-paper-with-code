# Word2Vec(Efficient Estimation of Word Representations in Vector Space)
> Paper : [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781)

## 1. Introduction

## 2. Model Architectures
### 2.1. Feedforward Neural Net Language Model (NNLM)

<p align="center">
    <img src="./img/NNLM.png" alt="NNLM">
</p>

### 2.2. Recurrent Neural Net Language Model (RNNLM)
### 2.3. Parallel Training of Neural Networks
## 3. New Log-linear Models

### 3.1. Continuous Bag-of-Words Model
### 3.2. Continuous Skip-gram Model
### 4. Results


## reference
- [NLP | 논문리뷰] Efficient Estimation of Word Representations in Vector Space : https://velog.io/@xuio/NLP-%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0-Sequence-to-Sequence-Learning-with-Neural-Networks-xikn77tw
-  네거티브 샘플링을 이용한 Word2Vec 구현(Skip-Gram with Negative Sampling, SGNS) : https://wikidocs.net/69141
- 피드 포워드 신경망 언어 모델(Neural Network Language Model, NNLM) : https://wikidocs.net/45609
